<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Wenfang Sun</title>

  <meta name="author" content="Wenfang Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenfang Sun(Â≠ôÊñáÊîæ)</name>
              </p>
              <p>
                I am a first-year Ph.D. student at the 
                <a href="https://www.uva.nl/en">University of Amsterdam (UvA)</a> in the 
                <a href="https://ivi.fnwi.uva.nl/vislab/">VIS Lab</a>, supervised by 
                <a href="https://www.ceessnoek.info/index.php/biography/">Prof. Dr. Cees Snoek</a> and 
                <a href="https://yingjundu.github.io/">Yingjun Du</a>. My research focuses on 
                <strong>multimodal foundation models</strong> as part of the Horizon Europe 
                <a href="https://ellis.eu/news/new-eu-project-elliot-aims-to-advance-multimodal-generalist-foundation-models">ELLIOT</a> project.
                </p>
                
                <p>
                I received my M.Sc. degree from the 
                <a href="https://en.ustc.edu.cn/">University of Science and Technology of China (USTC)</a>. 
                After graduation, I worked as a research assistant at 
                <a href="https://en.westlake.edu.cn/">Westlake University</a> with 
                <a href="https://scholar.google.com/citations?user=vAIECxgAAAAJ&hl=en">Prof. Yefeng Zheng</a>. 
                I also received valuable guidance from 
                <a href="https://scholar.google.com/citations?user=73IbXtsAAAAJ&hl=en">Dr. Shiwei Liu</a>, 
                <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Dr. Xiantong Zhen</a>, and 
                <a href="https://scholar.google.com/citations?user=NIv_aeQAAAAJ">Dr. Gaowen Liu</a>.
                </p>
                <p>
                  <font style="color: #FF0000;">Feel free to contact me for research collaborations.</font>
                </p>
		<p style="text-align:center">
                <a href="mailto:swf@mail.ustc.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/wenfang_cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=_pgNdU8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/lmsdss">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/lmsdss">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/wenfang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/wenfang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>  
              <p>[Sep 2024] One paper was accepted by <b>NeurIPS 2024</b>.
              </p>

              <p>[Apr 2024] One paper was accepted by <b>CVPR 2024 Workshop</b>.
              </p>

              <p>[Apr 2023] One paper was accepted by <b>ICML 2023</b>.
              </p>

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
          <tr onmouseout="submitted_stop()" onmouseover="submitted_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/curse.png' width="160">
              </div>
              <script type="text/javascript">
                function scaling_start() {
                  document.getElementById('scaling_main').style.opacity = "1";
                }
  
                function scaling_stop() {
                  document.getElementById('scaling_main').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2502.05795">
                <papertitle>The Curse of Depth in Large Language Models</papertitle>
              </a>
              <br>
              <strong>Wenfang Sun</strong>,
              Xinyuan Song,
              <a href="https://scholar.google.com/citations?user=rUp_4RgAAAAJ&hl=en">Pengxiang Li</a>,
              <a href="https://scholar.google.com/citations?user=G4Xe1NkAAAAJ&hl=en">Lu Yin</a>,
              <a href="https://scholar.google.com/citations?user=vAIECxgAAAAJ&hl=en">Yefeng Zheng</a>,
              <a href="https://scholar.google.com/citations?user=73IbXtsAAAAJ&hl=en">Shiwei Liu</a>

              <br>
              <em>Under Review</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2502.05795">paper</a> /
              <a href="https://github.com/lmsdss/LayerNorm-Scaling">code</a>
              <p></p>
              <p>We propose LayerNorm Scaling, a simple yet effective modification that mitigates the variance explosion in deep Transformer layers, enabling Large Language Models to fully leverage their depth and achieve consistently better pre-training and fine-tuning performance.</p>
              <p></p>
            </td>
          </tr>
          
          <tr onmouseout="submitted_stop()" onmouseover="submitted_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/quota.png' width="160">
              </div>
              <script type="text/javascript">
                function quota_start() {
                  document.getElementById('quota_main').style.opacity = "1";
                }

                function quota_stop() {
                  document.getElementById('quota_main').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2411.19534">
                <papertitle>QUOTA: Quantifying Objects with Text-to-Image Models for Any Domain</papertitle>
              </a>
              <br>
              <strong>Wenfang Sun</strong>,
              <a href="https://yingjundu.github.io/">Yingjun Du</a>,
              <a href="https://scholar.google.com/citations?user=NIv_aeQAAAAJ">Gaowen Liu</a>,
              <a href="https://scholar.google.com/citations?user=0uKdbscAAAAJ&hl=en">Cees G. M. Snoek</a>

              <br>
              <em>Under Review</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2411.19534">paper</a>
              <p></p>
              <p>We propose QUOTA, a domain-agnostic optimization framework for text-to-image models that enables accurate object quantification across unseen domains without retraining, by combining dual-loop meta-learning with prompt, counting, and domain tokens to achieve superior accuracy and adaptability.</p>
              <p></p>
            </td>
          </tr>
          
          <tr onmouseout="submitted_stop()" onmouseover="submitted_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <!-- <div class="two" id='PCCC_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/PCCC_supervision.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/nips2024.png' width="160">
              </div>
              <script type="text/javascript">
                function cvpr2023_start() {
                  document.getElementById('submitted_main').style.opacity = "1";
                }
  
                function cvpr2023_stop() {
                  document.getElementById('submitted_main').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2410.15397">
                <papertitle>IPO: Interpretable Prompt Optimization for Vision-Language Models</papertitle>
              </a>
              <br>
              <strong>Wenfang Sun<sup>*</strong>,
              <a href="https://scholar.google.com/citations?user=N1q5mfMAAAAJ&hl=zh-CN">Yingjun Du<sup>*</a>,
              <a href="https://scholar.google.com/citations?user=0uKdbscAAAAJ&hl=en">Cees Snoek</a>
  
              <br>
              <em>NeurIPS </em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2410.15397">paper</a> /
              <a href="https://github.com/lmsdss/IPO">code</a>
              <p></p>
              <p>We propose IPO, an interpretable prompt optimizer that uses LLMs to dynamically generate and refine prompts, while incorporating an LMM to enhance textual-visual interaction.</p>
              <p></p>
            </td>
          </tr>
          
         <tr onmouseout="submitted_stop()" onmouseover="submitted_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <!-- <div class="two" id='PCCC_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/PCCC_supervision.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/cvprws.png' width="160">
              </div>
              <script type="text/javascript">
                function cvpr2023_start() {
                  document.getElementById('submitted_main').style.opacity = "1";
                }

                function cvpr2023_stop() {
                  document.getElementById('submitted_main').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
							<a href="https://arxiv.org/abs/2404.00701">
                <papertitle>Training-Free Semantic Segmentation via LLM-Supervision</papertitle>
              </a>
              <br>
              <strong>Wenfang Sun<sup>*</strong>,
              <a href="https://scholar.google.com/citations?user=N1q5mfMAAAAJ&hl=zh-CN">Yingjun Du<sup>*</a>,
              <a href="https://scholar.google.com/citations?user=NIv_aeQAAAAJ">Gaowen Liu</a>,
              <a href="https://scholar.google.com/citations?user=uf9RZboAAAAJ">Ramana Rao Kompella</a>,
              <a href="https://scholar.google.com/citations?user=0uKdbscAAAAJ&hl=en">Cees Snoek</a>

              <br>
              <em>CVPR Workshop</em>, 2024
              <br>
							<a href="https://arxiv.org/abs/2404.00701">paper</a> 
              <p></p>
              <p>We propose a novel text-supervised semantic segmentation framework that leverages large language model supervision for enhanced class descriptors, refined subclass generation, and effective ensembling for improved segmentation accuracy.</p>
              <p></p>
            </td>
          </tr>


        


        <tr onmouseout="icml2023_stop()" onmouseover="icml2023_start()">
          <td style="padding:20px;width:25%;vertical-align:top">
            <div class="one">
              <!-- <div class="two" id='PCCC_image'><video  width=100% height=100% muted autoplay loop>
              <source src="images/PCCC_supervision.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div> -->
              <img src='images/icml2023_main.png' width="160">
            </div>
            <script type="text/javascript">
              function neurips2023_start() {
                document.getElementById('icml2023_main').style.opacity = "1";
              }

              function neurips2023_stop() {
                document.getElementById('icml2023_main').style.opacity = "0";
              }
              nerfsuper_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2305.10309">
              <papertitle>MetaModulation: Learning Variational Feature Hierarchies for Few-Shot Learning with Fewer Tasks</papertitle>
            </a>
            <br>
            <strong>Wenfang Sun<sup>*</strong>,
            <a href="https://scholar.google.com/citations?user=N1q5mfMAAAAJ&hl=zh-CN">Yingjun Du<sup>*</a>,
            <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong Zhen</a>,
            Fan Wang,
            Ling Wang,
            <a href="https://scholar.google.com/citations?user=0uKdbscAAAAJ&hl=en">Cees Snoek</a>
            <br>
            <em>ICML</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2305.10309">paper</a> /
            <a href="https://github.com/lmsdss/metamodulation">code</a>
            <p></p>
            <p>We propose a method for few-shot learning with fewer tasks, which we call MetaModulation.</p>
          </td>
        </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website is based on Jon's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:25%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px" width="60%">
              <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=cbf0ff&w=a&t=tt&d=C0sqYIEG6XzGCUryaJz_Ys3xp55LtJZJttqk4GMzO-Q&co=32aaff&cmo=00b92d&cmn=ff9500'></script>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </tbody></table>

</body>

</html>
